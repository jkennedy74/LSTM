{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  447851\n",
      "Total Vocab:  35\n",
      "Total Patterns:  447751\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#Just replace here to update the source.\n",
    "source = 'trumpsmall'\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename =  str(source)+\".txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
<<<<<<< HEAD:LSTM RNN.ipynb
    "seq_length = 50\n",
=======
    "seq_length = 100\n",
>>>>>>> notebook changes:LSTM Training Notebook.ipynb
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"checkpoint-{epoch:02d}-{loss:.4f}-\"+ source +\".hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "447751/447751 [==============================] - 4559s 10ms/step - loss: 2.5075\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.50745, saving model to checkpoint-01-2.5075-trumpsmall.hdf5\n",
      "Epoch 2/200\n",
      "447751/447751 [==============================] - 4775s 11ms/step - loss: 2.0183\n",
      "\n",
      "Epoch 00002: loss improved from 2.50745 to 2.01833, saving model to checkpoint-02-2.0183-trumpsmall.hdf5\n",
      "Epoch 3/200\n",
      "447751/447751 [==============================] - 4953s 11ms/step - loss: 1.8313\n",
      "\n",
<<<<<<< HEAD:LSTM RNN.ipynb
      "Epoch 00003: loss improved from 2.68923 to 2.46959, saving model to checkpoint-03-2.4696-mamet.hdf5\n",
      "Epoch 4/50\n",
      "77110/77110 [==============================] - 492s 6ms/step - loss: 2.3155\n",
      "\n",
      "Epoch 00004: loss improved from 2.46959 to 2.31550, saving model to checkpoint-04-2.3155-mamet.hdf5\n",
      "Epoch 5/50\n",
      "77110/77110 [==============================] - 486s 6ms/step - loss: 2.2072\n",
      "\n",
      "Epoch 00005: loss improved from 2.31550 to 2.20724, saving model to checkpoint-05-2.2072-mamet.hdf5\n",
      "Epoch 6/50\n",
      "77110/77110 [==============================] - 487s 6ms/step - loss: 2.1343\n",
      "\n",
      "Epoch 00006: loss improved from 2.20724 to 2.13431, saving model to checkpoint-06-2.1343-mamet.hdf5\n",
      "Epoch 7/50\n",
      "77110/77110 [==============================] - 489s 6ms/step - loss: 2.0712\n",
      "\n",
      "Epoch 00007: loss improved from 2.13431 to 2.07116, saving model to checkpoint-07-2.0712-mamet.hdf5\n",
      "Epoch 8/50\n",
      "77110/77110 [==============================] - 490s 6ms/step - loss: 2.0164\n",
      "\n",
      "Epoch 00008: loss improved from 2.07116 to 2.01639, saving model to checkpoint-08-2.0164-mamet.hdf5\n",
      "Epoch 9/50\n",
      "77110/77110 [==============================] - 492s 6ms/step - loss: 1.9724\n",
      "\n",
      "Epoch 00009: loss improved from 2.01639 to 1.97239, saving model to checkpoint-09-1.9724-mamet.hdf5\n",
      "Epoch 10/50\n",
      "77110/77110 [==============================] - 496s 6ms/step - loss: 1.9292\n",
      "\n",
      "Epoch 00010: loss improved from 1.97239 to 1.92924, saving model to checkpoint-10-1.9292-mamet.hdf5\n",
      "Epoch 11/50\n",
      "77110/77110 [==============================] - 497s 6ms/step - loss: 1.8936\n",
      "\n",
      "Epoch 00011: loss improved from 1.92924 to 1.89361, saving model to checkpoint-11-1.8936-mamet.hdf5\n",
      "Epoch 12/50\n",
      "77110/77110 [==============================] - 496s 6ms/step - loss: 1.8522\n",
      "\n",
      "Epoch 00012: loss improved from 1.89361 to 1.85225, saving model to checkpoint-12-1.8522-mamet.hdf5\n",
      "Epoch 13/50\n",
      "77110/77110 [==============================] - 497s 6ms/step - loss: 1.8255\n",
      "\n",
      "Epoch 00013: loss improved from 1.85225 to 1.82549, saving model to checkpoint-13-1.8255-mamet.hdf5\n",
      "Epoch 14/50\n",
      "77110/77110 [==============================] - 499s 6ms/step - loss: 1.7899\n",
      "\n",
      "Epoch 00014: loss improved from 1.82549 to 1.78989, saving model to checkpoint-14-1.7899-mamet.hdf5\n",
      "Epoch 15/50\n",
      "77110/77110 [==============================] - 500s 6ms/step - loss: 1.7616\n",
      "\n",
      "Epoch 00015: loss improved from 1.78989 to 1.76158, saving model to checkpoint-15-1.7616-mamet.hdf5\n",
      "Epoch 16/50\n",
      "77110/77110 [==============================] - 498s 6ms/step - loss: 1.7396\n",
      "\n",
      "Epoch 00016: loss improved from 1.76158 to 1.73964, saving model to checkpoint-16-1.7396-mamet.hdf5\n",
      "Epoch 17/50\n",
      "77110/77110 [==============================] - 503s 7ms/step - loss: 1.7096\n",
      "\n",
      "Epoch 00017: loss improved from 1.73964 to 1.70963, saving model to checkpoint-17-1.7096-mamet.hdf5\n",
      "Epoch 18/50\n",
      "77110/77110 [==============================] - 502s 7ms/step - loss: 1.6867\n",
      "\n",
      "Epoch 00018: loss improved from 1.70963 to 1.68671, saving model to checkpoint-18-1.6867-mamet.hdf5\n",
      "Epoch 19/50\n",
      "77110/77110 [==============================] - 506s 7ms/step - loss: 1.6646\n",
      "\n",
      "Epoch 00019: loss improved from 1.68671 to 1.66458, saving model to checkpoint-19-1.6646-mamet.hdf5\n",
      "Epoch 20/50\n",
      "77110/77110 [==============================] - 508s 7ms/step - loss: 1.6411\n",
      "\n",
      "Epoch 00020: loss improved from 1.66458 to 1.64110, saving model to checkpoint-20-1.6411-mamet.hdf5\n",
      "Epoch 21/50\n",
      "77110/77110 [==============================] - 525s 7ms/step - loss: 1.6189\n",
      "\n",
      "Epoch 00021: loss improved from 1.64110 to 1.61890, saving model to checkpoint-21-1.6189-mamet.hdf5\n",
      "Epoch 22/50\n",
      "57728/77110 [=====================>........] - ETA: 2:15 - loss: 1.6023"
=======
      "Epoch 00003: loss improved from 2.01833 to 1.83126, saving model to checkpoint-03-1.8313-trumpsmall.hdf5\n",
      "Epoch 4/200\n",
      " 80256/447751 [====>.........................] - ETA: 1:30:26 - loss: 1.7518"
>>>>>>> notebook changes:LSTM Training Notebook.ipynb
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200, batch_size=64, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD:LSTM RNN.ipynb
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-47-1.2219-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
=======
   "source": []
>>>>>>> notebook changes:LSTM Training Notebook.ipynb
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
